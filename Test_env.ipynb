{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wprowadzenie do sztucznej inteligencji - laboratorium - projekt zaliczeniowy \n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, filename,folder_path):\n",
    "    \"\"\"\n",
    "    Załadowanie danych.\n",
    "    @param path: Ścieżka do folderu z próbkami.\n",
    "    @param filename: Nazwa pliku csv, gdzie znajdują się informacje na temat próbki.\n",
    "    @return: Lista słowników, jeden dla każdej próbki -> próbka i jej klasa. \n",
    "    \"\"\"\n",
    "    entry_list = pandas.read_csv(os.path.join(path, filename))\n",
    "    data = []\n",
    "    for idx, entry in entry_list.iterrows():\n",
    "        if entry['Name'] == 'crosswalk':\n",
    "            class_id = 1\n",
    "        elif entry['Name'] == 'stop':\n",
    "            class_id = 2\n",
    "        elif entry['Name'] == 'speedlimit':\n",
    "            class_id = 2\n",
    "        elif entry['Name'] == 'trafficlight':\n",
    "            class_id = 2\n",
    "        \n",
    "        image_path = folder_path + entry['Path']\n",
    "\n",
    "        if class_id != -1:\n",
    "            image = cv2.imread(os.path.join(path, image_path))\n",
    "            data.append({'image': image, 'label': class_id})\n",
    "    return data \n",
    "\n",
    "def display_dataset_stats(data):\n",
    "    \"\"\"\n",
    "    Displays statistics about dataset in a form: class_id: number_of_samples\n",
    "    @param data: List of dictionaries, one for every sample, with entry \"label\" (class_id).\n",
    "    @return: Nothing\n",
    "    \"\"\"\n",
    "    class_to_num = {}\n",
    "    for idx, sample in enumerate(data):\n",
    "        class_id = sample['label']\n",
    "        if class_id not in class_to_num:\n",
    "            class_to_num[class_id] = 0\n",
    "        class_to_num[class_id] += 1\n",
    "\n",
    "    class_to_num = dict(sorted(class_to_num.items(), key=lambda item: item[0]))\n",
    "    print(class_to_num)\n",
    "\n",
    "def balance_dataset(data, ratio):\n",
    "    \"\"\"\n",
    "    Subsamples dataset according to ratio.\n",
    "    @param data: List of samples.\n",
    "    @param ratio: Ratio of samples to be returned.\n",
    "    @return: Subsampled dataset.\n",
    "    \"\"\"\n",
    "    sampled_data = random.sample(data, int(ratio * len(data)))\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def learn_bovw(data):\n",
    "    \"\"\"\n",
    "    Learns BoVW dictionary and saves it as \"voc.npy\" file.\n",
    "    @param data: List of dictionaries, one for every sample, with entries \"image\" (np.array with image) and \"label\" (class_id).\n",
    "    @return: Nothing\n",
    "    \"\"\"\n",
    "    dict_size = 128\n",
    "    bow = cv2.BOWKMeansTrainer(dict_size)\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    for sample in data:\n",
    "        kpts = sift.detect(sample['image'], None)\n",
    "        kpts, desc = sift.compute(sample['image'], kpts)\n",
    "\n",
    "        if desc is not None:\n",
    "            bow.add(desc)\n",
    "\n",
    "    vocabulary = bow.cluster()\n",
    "\n",
    "    np.save('voc.npy', vocabulary)\n",
    "\n",
    "def extract_features(data):\n",
    "    \"\"\"\n",
    "    Extracts features for given data and saves it as \"desc\" entry.\n",
    "    @param data: List of dictionaries, one for every sample, with entries \"image\" (np.array with image) and \"label\" (class_id).\n",
    "    @return: Data with added descriptors for each sample.\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create()\n",
    "    flann = cv2.FlannBasedMatcher_create()\n",
    "    bow = cv2.BOWImgDescriptorExtractor(sift, flann)\n",
    "    vocabulary = np.load('voc.npy')\n",
    "    bow.setVocabulary(vocabulary)\n",
    "    for sample in data:\n",
    "        # compute descriptor and add it as \"desc\" entry in sample\n",
    "        # TODO PUT YOUR CODE HERE\n",
    "        # Z ZAJĘĆ:\n",
    "        kpts = sift.detect(sample['image'], None)\n",
    "        desc = bow.compute(sample['image'], kpts)  # robienie deskryptora\n",
    "        sample['desc'] = desc\n",
    "        # ------------------\n",
    "\n",
    "    return data\n",
    "\n",
    "def train(data):  # tylko trenujemy model tutaj\n",
    "    \"\"\"\n",
    "    Trains Random Forest classifier.\n",
    "    @param data: List of dictionaries, one for every sample, with entries \"image\" (np.array with image), \"label\" (class_id),\n",
    "                    \"desc\" (np.array with descriptor).\n",
    "    @return: Trained model.\n",
    "    \"\"\"\n",
    "    # train random forest model and return it from function.\n",
    "    # TODO PUT YOUR CODE HERE\n",
    "    # GITHUB:\n",
    "\n",
    "    # Z ZAJEC:\n",
    "    descs = []\n",
    "    labels = []\n",
    "    for sample in data:\n",
    "        if sample['desc'] is not None:\n",
    "            descs.append(sample['desc'].squeeze(0))  # squeeze zmienia macierz na wektor wokol konkretnej osi -> tu wokol osi 0 (a mozemy wokol 0, 1 lub 2)\n",
    "            labels.append(sample['label'])\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(descs, labels)\n",
    "    # ------------------\n",
    "    # Z ZAJEC:\n",
    "    return rf  # wyjsciem funkcji jest model\n",
    "\n",
    "def predict(rf, data):  # przyjmuje rf gdzie mamy zapisany model i dane porzednie\n",
    "    \"\"\"\n",
    "    Predicts labels given a model and saves them as \"label_pred\" (int) entry for each sample.\n",
    "    @param rf: Trained model.\n",
    "    @param data: List of dictionaries, one for every sample, with entries \"image\" (np.array with image), \"label\" (class_id),\n",
    "                    \"desc\" (np.array with descriptor).\n",
    "    @return: Data with added predicted labels for each sample.\n",
    "    \"\"\"\n",
    "    # perform prediction using trained model and add results as \"label_pred\" (int) entry in sample\n",
    "    # TODO PUT YOUR CODE HERE\n",
    "    # Z ZAJEC:\n",
    "    for idx, sample in enumerate(data):\n",
    "        if sample['desc'] is not None:\n",
    "            pred = rf.predict(sample['desc'])  # ta linia jest kluczowa dla predykcji, ale my chcemy zewaluowac cala baze danych dlatego robimy inne linijki\n",
    "            sample['label_pred'] = int(pred)\n",
    "    # zwraca etykiete do pred i uzupelniamy tabele data etykietą (etykiety byly 1, 2 ,3)\n",
    "    # ------------------\n",
    "\n",
    "    return data  # dane z wypredykowanymi etykietami\n",
    "\n",
    "def evaluate(data):  # porownanie statystyczne, kolumna label_pred - wypredkowane labele, a w kolumnie label - etykiety prawdziwe. Wykorzystujemy jedna z metryk ewaluacji\n",
    "    \"\"\"\n",
    "    Evaluates results of classification.\n",
    "    @param data: List of dictionaries, one for every sample, with entries \"image\" (np.array with image), \"label\" (class_id),\n",
    "                    \"desc\" (np.array with descriptor), and \"label_pred\".\n",
    "    @return: Nothing.\n",
    "    \"\"\"\n",
    "    # evaluate classification results and print statistics\n",
    "    # TODO PUT YOUR CODE HERE\n",
    "    # Accuracy, ile rzeczy model trafil -> pierwsza z metod ewaluacji (wszystkie proby -> mianownik, to co sie udal -> w liczniku)\n",
    "    # Z ZAJEC:\n",
    "    n_corr = 0\n",
    "    n_incorr = 0\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    for idx, sample in enumerate(data):\n",
    "        if sample['desc'] is not None:\n",
    "            pred_labels.append(sample['label_pred'])\n",
    "            true_labels.append(sample['label'])\n",
    "            if sample['label_pred'] == sample['label']:\n",
    "                n_corr += 1\n",
    "            else:\n",
    "                n_incorr += 1\n",
    "    n = n_corr / max(n_corr + n_incorr, 1)\n",
    "    print(\"Score = \" + str(n))\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # ------------------\n",
    "    # ------------------\n",
    "\n",
    "    # this function does not return anything\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Dane treningowe ###\n",
      "Wczytywanie danych treningowych.\n",
      "Statystyka przed balansowaniem:\n",
      "{1: 20, 2: 60}\n",
      "Statystyka po balansowaniu:\n",
      "{1: 20, 2: 60}\n",
      "### Dane testowe ###\n",
      "Wczytywanie danych testowych.\n",
      "Dane testowe przed balansowaniem:\n",
      "{1: 100, 2: 697}\n",
      "Dane testowe po balansowaniu:\n",
      "{1: 100, 2: 697}\n",
      "extracting train features\n",
      "training\n",
      "extracting test features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17788/1665891722.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17788/1665891722.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'extracting test features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'testing on testing dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17788/2264682322.py\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# Z ZAJĘĆ:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mkpts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkpts\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# robienie deskryptora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'desc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# ------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"### Dane treningowe ###\")\n",
    "    print(\"Wczytywanie danych treningowych.\")\n",
    "    data_train = load_data('./','Train.csv','./')\n",
    "    print('Statystyka przed balansowaniem:')\n",
    "    display_dataset_stats(data_train)\n",
    "    data_train = balance_dataset(data_train, 1.0)\n",
    "    print('Statystyka po balansowaniu:')\n",
    "    display_dataset_stats(data_train)\n",
    "\n",
    "    print(\"### Dane testowe ###\")\n",
    "    print(\"Wczytywanie danych testowych.\")\n",
    "    data_test = load_data('./', 'Test.csv','./Test/')\n",
    "    print('Dane testowe przed balansowaniem:')\n",
    "    display_dataset_stats(data_test)\n",
    "    data_test = balance_dataset(data_test, 1.0)\n",
    "    print('Dane testowe po balansowaniu:')\n",
    "    display_dataset_stats(data_test)\n",
    "\n",
    "    # you can comment those lines after dictionary is learned and saved to disk.\n",
    "    # print('learning BoVW')\n",
    "    # learn_bovw(data_train)\n",
    "\n",
    "    print('extracting train features')\n",
    "    data_train = extract_features(data_train)\n",
    "\n",
    "    print('training')\n",
    "    rf = train(data_train)\n",
    "\n",
    "    print('extracting test features')\n",
    "    data_test = extract_features(data_test)\n",
    "\n",
    "    print('testing on testing dataset')\n",
    "    data_test = predict(rf, data_test)\n",
    "    evaluate(data_test)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(data):\n",
    "    \"\"\"\n",
    "    Displays samples of correct and incorrect classification.\n",
    "    @param data: List of dictionaries, one for every sample, with entries \"image\" (np.array with image), \"label\" (class_id),\n",
    "                    \"desc\" (np.array with descriptor), and \"label_pred\".\n",
    "    @return: Nothing.\n",
    "    \"\"\"\n",
    "    n_classes = 3\n",
    "\n",
    "    corr = {}\n",
    "    incorr = {}\n",
    "    for idx, sample in enumerate(data):\n",
    "        if sample['desc'] is not None:\n",
    "            print(sample['label'])\n",
    "    # this function does not return anything\n",
    "    return\n",
    "\n",
    "display(data_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22fa8cc1021e09c5b0585096e0b20bfac1a79adbd55254c2f0263228728dcf19"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
